{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1638222162922
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Added cell to measure execution time\r\n",
    "import time\r\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run distributed:\n",
    "\n",
    "```\n",
    "make\n",
    "```\n",
    "\n",
    "```\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --master_port 1235 --nproc_per_node=4 02_train.py --epochs 10 --bs 48 --fp16 to_fp16 --trf_heads 4 --mixup False --chunk_size 500 --trf_dim 512 --loss ce --n_chunks 1 --fit fit_flat_cos --fit_kwargs pct_start=0.5 div_final=100 --tfixup True --pad r --valid_pct 0.025 --trf_act gelu --opt ranger_lamb --lr 3e-3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1638222163630
    }
   },
   "outputs": [],
   "source": [
    "import fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1638222188034
    }
   },
   "outputs": [],
   "source": [
    "from fastai.basics       import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.distributed  import *\n",
    "from fastai.tabular.all  import *\n",
    "from fastai.test_utils   import *\n",
    "\n",
    "import ast\n",
    "import enum\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import enum\n",
    "\n",
    "from collections import defaultdict\n",
    "from fastcore.script import *\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from pytorch_block_sparse.util import ModelPatcher\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.distributions.beta import Beta\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1638222188500
    }
   },
   "outputs": [],
   "source": [
    "in_d = Path('../input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1638222189118
    }
   },
   "outputs": [],
   "source": [
    "@call_parse\n",
    "def main(\n",
    "    model:         Param(\"Name\", str) = '210105',\n",
    "    data:          Param(\"Data version\", str) = '210101b',\n",
    "    load:          Param(\"Load from\", str) = None,\n",
    "    validate:      Param(\"\", action='store_true') = False,\n",
    "    chunk_size:    Param(\"Chunk size\", int) = 500,\n",
    "    n_chunks:      Param(\"Number of chunks\", int) = 1,\n",
    "    #chunk_size:    Param(\"Chunk size\", int) = 50,  \n",
    "    #n_chunks:      Param(\"Number of chunks\", int) = 10,    \n",
    "    #bs:            Param(\"BS\", int) = 96,\n",
    "    bs:            Param(\"BS\", int) = 48,    #2 works, 96 doesn't\n",
    "    workers:       Param(\"\", int) = 8,\n",
    "    valid_pct:     Param(\"Validation set\", float) = 0.025,\n",
    "    trf_dim:       Param(\"\", int) = 512,\n",
    "    trf_enc:       Param(\"\", int) = 4,\n",
    "    trf_dec:       Param(\"\", int) = 4,\n",
    "    trf_heads:     Param(\"\", int) = 4,\n",
    "    trf_do:        Param(\"\", float) = 0.1,\n",
    "    trf_act:       Param(\"\", str) = 'gelu',\n",
    "    lr:            Param(\"\", float) = 3e-3,\n",
    "    clip:          Param(\"\", float) = 0.,\n",
    "    \n",
    "    moms:          Param(\"Moms for fit_one_cycle\", float, nargs='+') = (0.95,0.85,0.95),\n",
    "    epochs:        Param(\"Epochs\", int) = 10, #Original was 30,\n",
    "    tfixup:        Param(\"Use T-Fixup init\", ast.literal_eval) = True,\n",
    "    mixup:         Param(\"Use mixup\", ast.literal_eval) = False,\n",
    "    opt:           Param(\"Optimizer\", str) = 'ranger_lamb',\n",
    "    opt_kwargs:    Param(\"Optional args for opt, eg. eps=1e-4\", str, nargs='+') = {},\n",
    "    fit:           Param(\"fit or fit_one_cycle\", str) = 'fit_flat_cos',\n",
    "    fit_kwargs:    Param(\"Optional args for fit,eg pct_start=0.1\", str, nargs='+') = ['pct_start=0.5', 'div_final=100.'],\n",
    "    fp16:          Param(\"fp16 method: to_fp16, to_native_fp16, none\", str) = 'to_fp16',\n",
    "    \n",
    "    loss:          Param(\"Loss\", str) = 'ce',\n",
    "    \n",
    "    wua:           Param(\"Weight of user_answer term in the loss\", float) = 0.,\n",
    "    pad:           Param (\"Pad left of right (l|r)\",str,choices=['l','r'])='r',\n",
    "\n",
    "    local_rank:    Param(\"--local_rank\", int) = None,\n",
    "): \n",
    "    if opt_kwargs: opt_kwargs = {s.split('=')[0]:float(s.split('=')[1]) for s in opt_kwargs}\n",
    "    if fit_kwargs: fit_kwargs = {s.split('=')[0]:float(s.split('=')[1]) for s in fit_kwargs}\n",
    "    print(locals())\n",
    "    globals().update({ 'H' : AttrDict(locals())})\n",
    "_H = AttrDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1638222189602
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': '210105', 'data': '210101b', 'load': None, 'validate': False, 'chunk_size': 500, 'n_chunks': 1, 'bs': 48, 'workers': 8, 'valid_pct': 0.025, 'trf_dim': 512, 'trf_enc': 4, 'trf_dec': 4, 'trf_heads': 4, 'trf_do': 0.1, 'trf_act': 'gelu', 'lr': 0.003, 'clip': 0.0, 'moms': (0.95, 0.85, 0.95), 'epochs': 10, 'tfixup': True, 'mixup': False, 'opt': 'ranger_lamb', 'opt_kwargs': {}, 'fit': 'fit_flat_cos', 'fit_kwargs': {'pct_start': 0.5, 'div_final': 100.0}, 'fp16': 'to_fp16', 'loss': 'ce', 'wua': 0.0, 'pad': 'r', 'local_rank': None}\n"
     ]
    }
   ],
   "source": [
    "#noexport\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1638222190098
    }
   },
   "outputs": [],
   "source": [
    "n_gpus = torch.cuda.device_count() if H.local_rank is None else 1\n",
    "if H.local_rank is not None:\n",
    "    torch.cuda.set_device(H.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "    print(f\"DISTRIBUTED: {H.local_rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read df and meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1638222190561
    }
   },
   "outputs": [],
   "source": [
    "#TODO detect if meta exists and don't load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1638222191008
    }
   },
   "outputs": [],
   "source": [
    "with open(in_d / f'meta_v{H.data}.pkl', 'rb') as f:\n",
    "    meta = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1638222191440
    }
   },
   "outputs": [],
   "source": [
    "QCols = enum.IntEnum('QCols', meta.qcols, start=0)\n",
    "LCols = enum.IntEnum('LCols', meta.lcols, start=0)\n",
    "Cats  = enum.IntEnum('Cats',  meta.cat_names, start=0)\n",
    "Conts = enum.IntEnum('Conts', meta.cont_names, start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222428038
    }
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "with open(in_d / f'data_v{H.data}.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222428781
    }
   },
   "outputs": [],
   "source": [
    "#TODO detect if data exists and don't load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222429271
    }
   },
   "outputs": [],
   "source": [
    "del data.attempt_num_coo\n",
    "del data.attempts_correct_coo\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222440022
    }
   },
   "outputs": [],
   "source": [
    "lut = meta.icats['answered_correctly'],meta.icats['user_answer']\n",
    "y_d = {}\n",
    "for k, v in data.cat_d.items():\n",
    "    y_d[k] = np.column_stack((lut[0][v[:,Cats.answered_correctly] - 1],lut[1][v[:,Cats.user_answer] - 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chop sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222440495
    }
   },
   "outputs": [],
   "source": [
    "def chop_sequence(d):\n",
    "    nv = defaultdict(dict)\n",
    "    for k, v in d.items():\n",
    "        i = 0\n",
    "        while i*H.chunk_size < len(v):\n",
    "            nv[k][i] = v[i*H.chunk_size:(i+1)*H.chunk_size]\n",
    "            i += 1\n",
    "    return nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222452452
    }
   },
   "outputs": [],
   "source": [
    "cat_d  = chop_sequence(data.cat_d)\n",
    "cont_d = chop_sequence(data.cont_d)\n",
    "tags_d = chop_sequence(data.tags_d)\n",
    "tagw_d = chop_sequence(data.tagw_d)\n",
    "y_d    = chop_sequence(y_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222452997
    }
   },
   "outputs": [],
   "source": [
    "#assert np.concatenate(list(cat_d.values())).shape[0] == np.concatenate(list(data.cat_d.values())).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222453655
    }
   },
   "outputs": [],
   "source": [
    "print(f'There are {len(data.cat_d)} different users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222454133
    }
   },
   "outputs": [],
   "source": [
    "group_keys = sorted(list(cat_d.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222454598
    }
   },
   "outputs": [],
   "source": [
    "# Last H.valid_pct is valid set\n",
    "train_group_keys = group_keys[:int((1 - H.valid_pct) * len(group_keys))]\n",
    "valid_group_keys = group_keys[int((1 - H.valid_pct) * len(group_keys)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222455195
    }
   },
   "outputs": [],
   "source": [
    "print(f'users: train={len(train_group_keys)}, valid={len(valid_group_keys)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222455688
    }
   },
   "outputs": [],
   "source": [
    "def split_dict(d, keys):\n",
    "    return { (u, t): d[u][t] for u in keys for t in d[u].keys() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222456219
    }
   },
   "outputs": [],
   "source": [
    "train_x_cat =  split_dict(cat_d, train_group_keys)\n",
    "train_x_cont = split_dict(cont_d, train_group_keys)\n",
    "train_x_tags = split_dict(tags_d, train_group_keys)\n",
    "train_x_tagw = split_dict(tagw_d, train_group_keys)\n",
    "train_y =      split_dict(y_d, train_group_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222456722
    }
   },
   "outputs": [],
   "source": [
    "valid_x_cat =  split_dict(cat_d, valid_group_keys)\n",
    "valid_x_cont = split_dict(cont_d, valid_group_keys)\n",
    "valid_x_tags = split_dict(tags_d, valid_group_keys)\n",
    "valid_x_tagw = split_dict(tagw_d, valid_group_keys)\n",
    "valid_y =      split_dict(y_d, valid_group_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222457205
    }
   },
   "outputs": [],
   "source": [
    "print(f'seqs: train={len(train_x_cat)}, valid={len(valid_x_cat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222457943
    }
   },
   "outputs": [],
   "source": [
    "class InteractionsDataset(Dataset):\n",
    "    def __init__(self, x_cat, x_cont, x_tags, x_tagw, y, minids=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.means = np.expand_dims(meta.means, axis=0) # ready to broadcast\n",
    "        self.stds  = np.expand_dims(meta.stds , axis=0)\n",
    "        \n",
    "        self.n_inp = 5  # number of feature (x) tensors\n",
    "        \n",
    "        self.x_cat = x_cat  # SL, XF (sequence len, feature columns) \n",
    "        self.x_cont = x_cont\n",
    "        self.x_tags = x_tags      \n",
    "        self.x_tagw = x_tagw\n",
    "        self.y = y  # SL, 1\n",
    "        \n",
    "        self.keys = list(self.x_cat.keys()) # list of group keys\n",
    "        \n",
    "        if minids:\n",
    "            self.keys = self.keys[:H.bs*2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys) # H.bs * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id, time_slice = self.keys[idx]\n",
    "        win = range(max(0, time_slice - H.n_chunks + 1), time_slice + 1)\n",
    "        x_cat  = np.concatenate([ self.x_cat [(user_id, ts)] for ts in win ])\n",
    "        x_cont = np.concatenate([ self.x_cont[(user_id, ts)] for ts in win ])\n",
    "        x_tags = np.concatenate([ self.x_tags[(user_id, ts)] for ts in win ])\n",
    "        x_tagw = np.concatenate([ self.x_tagw[(user_id, ts)] for ts in win ])\n",
    "        y      = np.concatenate([ self.y     [(user_id, ts)] for ts in win ])\n",
    "        \n",
    "        pad = H.chunk_size * H.n_chunks - x_cat.shape[0]\n",
    "        \n",
    "        # Normalize x_cont\n",
    "        x_cont = (x_cont - self.means) / self.stds\n",
    "        x_cont[np.isnan(x_cont)] = 0\n",
    "        \n",
    "        padt = (0,pad) if H.pad == 'r' else (pad,0)\n",
    "        \n",
    "        x_mask = np.zeros(x_cat.shape[0], dtype=np.bool)\n",
    "        \n",
    "        x_mask = np.pad(x_mask, padt, constant_values=(True))\n",
    "        x_cat  = np.pad(x_cat , (padt, (0, 0)), constant_values=(0)).astype(np.int64)\n",
    "        x_cont = np.pad(x_cont, (padt, (0, 0)), constant_values=(0)).astype(np.float32)\n",
    "        x_tags = np.pad(x_tags, (padt, (0, 0)), constant_values=(0)).astype(np.int64)\n",
    "        x_tagw = np.pad(x_tagw, (padt, (0, 0)), constant_values=(0.)).astype(np.float32)\n",
    "        y      = np.pad(y,      (padt, (0, 0)), constant_values=(-1)).astype(np.int64)\n",
    "\n",
    "        return x_mask, x_cat, x_cont, x_tags, x_tagw, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222458442
    }
   },
   "outputs": [],
   "source": [
    "train_ds = InteractionsDataset(train_x_cat, train_x_cont, train_x_tags, train_x_tagw, train_y)\n",
    "valid_ds = InteractionsDataset(valid_x_cat, valid_x_cont, valid_x_tags, valid_x_tagw, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222458908
    }
   },
   "outputs": [],
   "source": [
    "x_mask, x_cat, x_cont, x_tags, x_tagw, y = train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222459455
    }
   },
   "outputs": [],
   "source": [
    "len(train_ds.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222459943
    }
   },
   "outputs": [],
   "source": [
    "#x_tagw[-47:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222460406
    }
   },
   "outputs": [],
   "source": [
    "assert x_cat.shape == (H.chunk_size*H.n_chunks, len(meta.cat_names))\n",
    "assert x_cont.shape == (H.chunk_size*H.n_chunks, len(meta.cont_names))\n",
    "assert x_tags.shape == x_tagw.shape == (H.chunk_size*H.n_chunks, 6)\n",
    "assert y.shape == (H.chunk_size*H.n_chunks, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222460868
    }
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs=H.bs, shuffle=True, drop_last=True, num_workers=H.workers)\n",
    "valid_dl = DataLoader(valid_ds, bs=H.bs,                               num_workers=H.workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222461346
    }
   },
   "outputs": [],
   "source": [
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222461856
    }
   },
   "outputs": [],
   "source": [
    "x_mask,x_cat, x_cont, x_tags, x_tagw, y = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222465470
    }
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)\n",
    "torch.set_printoptions(precision=4, linewidth=200, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222465922
    }
   },
   "outputs": [],
   "source": [
    "x_cont[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222466331
    }
   },
   "outputs": [],
   "source": [
    "x_cat.shape, x_cont.shape, x_tags.shape, x_tagw.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222466678
    }
   },
   "outputs": [],
   "source": [
    "assert x_cat.isnan().any() == False\n",
    "assert x_cont.isnan().any() == False\n",
    "assert x_tags.isnan().any() == False\n",
    "assert x_tagw.isnan().any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222466990
    }
   },
   "outputs": [],
   "source": [
    "assert x_cat.shape == (H.bs, H.chunk_size*H.n_chunks, len(meta.cat_names))\n",
    "assert x_cont.shape == (H.bs, H.chunk_size*H.n_chunks, len(meta.cont_names))\n",
    "assert x_tags.shape == x_tagw.shape == (H.bs, H.chunk_size*H.n_chunks, 6)\n",
    "assert y.shape == (H.bs, H.chunk_size*H.n_chunks, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222467477
    }
   },
   "outputs": [],
   "source": [
    "def roc_auc(pred, targ):\n",
    "    pred = torch.softmax(pred, dim=2)\n",
    "    pred = pred[:,:,1:2] # prediction for True\n",
    "    idx = targ != -1\n",
    "    pred = pred[idx]\n",
    "    targ = targ[idx]\n",
    "    pred, targ = flatten_check(pred, targ)\n",
    "    if len(targ.unique()) == 2:\n",
    "        return roc_auc_score(targ.cpu().numpy(), pred.cpu().numpy())\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222467792
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss if H.loss=='ce' else globals()[H.loss]\n",
    "loss    = loss_fn(ignore_index=-1)\n",
    "loss_nr = loss_fn(ignore_index=-1, reduction='none')\n",
    "\n",
    "def loss_func(pred, targ, shuffle=None, lam=None):\n",
    "    b, s, l = pred.shape\n",
    "    if shuffle is not None:\n",
    "        targ_shuffled = targ[shuffle].view(b*s)\n",
    "    pred = pred.view(b*s, l)\n",
    "    targ = targ.view(b*s)\n",
    "\n",
    "    if shuffle is not None:\n",
    "        l0 = loss_nr(pred, targ).view(b, s)\n",
    "        l1 = loss_nr(pred, targ_shuffled).view(b, s)\n",
    "        return torch.lerp(l0, l1, lam.view(lam.shape[0], 1)).mean()\n",
    "    else:\n",
    "        #print(targ.unique()) # CUDA assert error if any index here is bigger than dimension l (labels) of pred\n",
    "        return loss(pred, targ)\n",
    "    \n",
    "def ua_loss_func(pred, targ, shuffle=None, lam=None):\n",
    "    loss_fn = loss_func\n",
    "    l = loss_fn(pred[...,:2],targ[...,:1],shuffle,lam) \n",
    "    if H.wua and targ.shape[-1]>1: l += H.wua * loss_fn(pred[...,2:],targ[...,1:],shuffle,lam)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222468095
    }
   },
   "outputs": [],
   "source": [
    "#noexport\n",
    "_p = torch.zeros([32, 127, 6])\n",
    "_t = torch.empty ([32, 127,2]).type(torch.long)\n",
    "_t[...,0] = torch.randint(2,_t.shape[:2])\n",
    "_t[...,1] = torch.randint(4,_t.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222468399
    }
   },
   "outputs": [],
   "source": [
    "#noexport\n",
    "roc_auc(_p[...,:2], _t[...,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222468782
    }
   },
   "outputs": [],
   "source": [
    "#noexport\n",
    "loss_func(_p[...,:2], _t[...,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222469093
    }
   },
   "outputs": [],
   "source": [
    "#noexport\n",
    "ua_loss_func(_p, _t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222469378
    }
   },
   "outputs": [],
   "source": [
    "class LBMetric(Metric):\n",
    "    def __init__(self, loss_func, name):\n",
    "        self.loss_func = loss_func\n",
    "        self.nam = name\n",
    "        \n",
    "    def reset(self):\n",
    "        self.targs, self.preds = [], []\n",
    "        \n",
    "    def accumulate(self, learn):\n",
    "        self.preds.append(learn.to_detach(learn.pred[...,:2]))\n",
    "        self.targs.append(learn.to_detach(learn.y[...,:1]))\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        if len(self.preds) == 0: return\n",
    "        preds = torch.cat(self.preds)\n",
    "        targs = torch.cat(self.targs)\n",
    "        r = self.loss_func(preds, targs)\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.nam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222469870
    }
   },
   "outputs": [],
   "source": [
    "#noexport\n",
    "lam = Beta(0.5, 0.5).sample((10000,))\n",
    "lam = torch.stack([lam, 1-lam], 1)\n",
    "lam = lam.max(1)[0].numpy()\n",
    "_ = plt.hist(lam, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222470181
    }
   },
   "outputs": [],
   "source": [
    "class MyMixUp(Callback):\n",
    "    run_after,run_valid = [Normalize],False\n",
    "    def __init__(self, alpha=0.4): \n",
    "        self.distrib = Beta(tensor(alpha), tensor(alpha))\n",
    "\n",
    "    def before_batch(self):\n",
    "        lam = self.distrib.sample((self.y.size(0),)).squeeze().to(self.y.device)\n",
    "        lam = torch.stack([lam, 1-lam], 1)\n",
    "        lam = lam.max(1)[0]\n",
    "        shuffle = torch.randperm(self.y.size(0)).to(self.y.device)\n",
    "        self.learn.xb = (*self.xb, shuffle, lam)\n",
    "        self.learn.yb = (*self.yb, shuffle, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222470520
    }
   },
   "outputs": [],
   "source": [
    "class GradientClipping(Callback):\n",
    "    \"Gradient clipping during training.\"\n",
    "    def __init__(self, clip:float = 0.):\n",
    "        self.clip = clip\n",
    "\n",
    "    def after_backward(self, **kwargs):\n",
    "        \"Clip the gradient before the optimizer step.\"\n",
    "        if self.clip: nn.utils.clip_grad_norm_(self.learn.model.parameters(), self.clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222470923
    }
   },
   "outputs": [],
   "source": [
    "class TutorNet(nn.Module):\n",
    "    def __init__(self, emb_szs, tag_emb_szs, emb_do, n_cont, trf_dim, trf_enc, trf_dec, trf_heads, trf_do, trf_act):\n",
    "        super().__init__()\n",
    "        self.nhead,self.trf_dim = trf_heads, trf_dim\n",
    "        \n",
    "        tag_emb_szs =(tag_emb_szs[0]+1, trf_dim)\n",
    "\n",
    "        self.embeds    = nn.ModuleList([nn.Sequential(nn.Embedding(ni+1, nf, max_norm=1.),nn.Linear(nf, trf_dim)) \n",
    "                                        for ni, nf in emb_szs])\n",
    "        self.tagembeds = nn.EmbeddingBag(*tag_emb_szs, max_norm=1., mode='sum')\n",
    "        self.conts     = nn.Linear(n_cont, trf_dim)\n",
    "            \n",
    "        self.trafo = nn.Transformer(\n",
    "            d_model = trf_dim,\n",
    "            nhead = trf_heads,\n",
    "            num_encoder_layers = trf_enc,\n",
    "            num_decoder_layers = trf_dec,\n",
    "            dim_feedforward = trf_dim*4,\n",
    "            dropout = trf_do,\n",
    "            activation = trf_act,\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Linear(trf_dim, 6)\n",
    "        \n",
    "    def forward(self, x_mask, x_cat, x_cont, x_tags, x_tagw, shuffle=None, lam=None):\n",
    "        b, sl, catf, contf, tagsf = (*x_cat.shape, x_cont.shape[2], x_tags.shape[2])\n",
    "        \n",
    "        x_cat  += 1\n",
    "        x_tags += 1\n",
    "    \n",
    "        # compute masks\n",
    "        causal_mask  = torch.triu(torch.ones(1,sl, sl,dtype=torch.bool,device=x_cat.device), diagonal=1).expand(b,-1,-1)\n",
    "        x_tci   = x_cat[...,Cats.task_container_id]\n",
    "        x_tci_s = torch.zeros_like(x_tci)\n",
    "        x_tci_s[...,1:] = x_tci[...,:-1]\n",
    "        enc_container_aware_mask =  (x_tci.unsqueeze(-1) == x_tci_s.unsqueeze(-1).permute(0,2,1)) | causal_mask\n",
    "        dec_container_aware_mask = ~(x_tci.unsqueeze(-1) == x_tci.unsqueeze(-1).permute(0,2,1))   & causal_mask\n",
    "\n",
    "        padding_mask = x_mask \n",
    "                \n",
    "        # encoder x (shifted q & a)\n",
    "        enc_cat  = torch.zeros_like(x_cat)\n",
    "        enc_cont = torch.zeros_like(x_cont)\n",
    "        enc_tags = torch.zeros_like(x_tags)\n",
    "        enc_tagw = torch.zeros_like(x_tagw)\n",
    "        \n",
    "        enc_cat[:,1:]  = x_cat[:,:-1]\n",
    "        enc_cont[:,1:] = x_cont[:,:-1]\n",
    "        enc_tags[:,1:] = x_tags[:,:-1]\n",
    "        enc_tagw[:,1:] = x_tagw[:,:-1]\n",
    "        \n",
    "        # decoder x (nonshifted q)\n",
    "        dec_cat  = x_cat\n",
    "        dec_cont = x_cont\n",
    "        dec_tags = x_tags\n",
    "        dec_tagw = x_tagw\n",
    "\n",
    "        # hide correct answer and user answered correctly from decoder\n",
    "        dec_cat[...,Cats.answered_correctly] = 0\n",
    "        dec_cat[...,Cats.user_answer] = 0\n",
    "        dec_cat[...,Cats.qhe] = 0\n",
    "        dec_cont[...,Conts.qet] = 0\n",
    "        dec_cont[...,Conts.qet_log] = 0\n",
    "        \n",
    "        # print(enc_cont.shape)\n",
    "        enc_cat  =  enc_cat.view(b * sl, catf)   # b*sl, catf\n",
    "        enc_tags = enc_tags.view(b * sl, tagsf) # b*sl, tagsf\n",
    "        enc_tagw = enc_tagw.view(b * sl, tagsf) # b*sl, tagsf\n",
    "\n",
    "        dec_cat  =  dec_cat.view(b * sl, catf)   # b*sl, catf\n",
    "        dec_tags = dec_tags.view(b * sl, tagsf) # b*sl, tagsf\n",
    "        dec_tagw = dec_tagw.view(b * sl, tagsf) # b*sl, tagsf\n",
    "        \n",
    "        # embed categorical vars\n",
    "        enc = torch.mean(torch.stack([\n",
    "            *[ e(enc_cat[:,i]) for i, e in enumerate(self.embeds) ],\n",
    "            self.tagembeds(enc_tags, per_sample_weights=enc_tagw),\n",
    "            self.conts(enc_cont).view(-1,self.trf_dim)\n",
    "        ]),dim=0)\n",
    "        \n",
    "        dec = torch.mean(torch.stack([\n",
    "            *[ e(dec_cat[:,i]) for i, e in enumerate(self.embeds) ],\n",
    "            self.tagembeds(dec_tags, per_sample_weights=dec_tagw),\n",
    "            self.conts(dec_cont).view(-1,self.trf_dim)\n",
    "        ]),dim=0)\n",
    "        \n",
    "        enc = enc.view(b, sl, self.trf_dim)           # b, sl, sum of cat, cont and tag ftrs\n",
    "        dec = dec.view(b, sl, self.trf_dim)           # b, sl, sum of cat, cont and tag ftrs\n",
    "\n",
    "        if shuffle is not None:\n",
    "            enc = torch.lerp(enc, enc[shuffle], lam.view(lam.shape[0], 1, 1))\n",
    "            dec = torch.lerp(dec, dec[shuffle], lam.view(lam.shape[0], 1, 1))\n",
    "            padding_mask = None\n",
    "            enc_container_aware_mask = dec_container_aware_mask = causal_mask | causal_mask[shuffle]\n",
    "        \n",
    "        enc = enc.permute(1, 0, 2)          # sl, b, tf (torchformer input)\n",
    "        dec = dec.permute(1, 0, 2)          # sl, b, tf\n",
    "\n",
    "        expand_nheads = lambda t: t.unsqueeze(1).expand(t.shape[0],self.nhead,-1,-1).reshape(-1,*t.shape[-2:])\n",
    "        \n",
    "        o = self.trafo(\n",
    "            enc, \n",
    "            dec, \n",
    "            src_mask = expand_nheads(enc_container_aware_mask),\n",
    "            tgt_mask = expand_nheads(dec_container_aware_mask),\n",
    "            memory_mask = expand_nheads(enc_container_aware_mask),\n",
    "            src_key_padding_mask = padding_mask,\n",
    "            tgt_key_padding_mask = padding_mask,\n",
    "            memory_key_padding_mask = padding_mask,\n",
    "        )                                   # sl, b, tf\n",
    "        o = o.permute(1, 0, 2)              # b, sl, tf\n",
    "        o = self.mlp(o)                     # b, sl, of (of=2)\n",
    "        #print(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222471238
    }
   },
   "outputs": [],
   "source": [
    "emb_szs = list(zip(meta.n_emb.values(), meta.emb_dim.values()))\n",
    "tag_emb_szs = meta.tags_n_emb, meta.tags_emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222471563
    }
   },
   "outputs": [],
   "source": [
    "model = TutorNet(emb_szs, tag_emb_szs, None, len(meta.cont_names), \n",
    "                 H.trf_dim, H.trf_enc, H.trf_dec, H.trf_heads, H.trf_do, H.trf_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-Fixup init\n",
    "\n",
    "1. Apply Xavier initialization for all parameters excluding input embeddings. Use Gaussian initialization $N(0,d^{-\\frac{1}{2}})$ for input embeddings where d is the embedding dimension.\n",
    "\n",
    "2. Scale $v_{d}$ and $w_{d}$ matrices in each decoder attention block, weight matrices in each decoder MLP block and input embeddings $x$ and $y$ in encoder and decoder by $(9N)^{−\\frac{1}{4}}$: [code](https://github.com/layer6ai-labs/T-Fixup/blob/f1fae213ce7b48829f81632d0c96bb039b7c450e/fairseq/modules/transformer_layer.py#L161), [code](https://github.com/layer6ai-labs/T-Fixup/blob/f1fae213ce7b48829f81632d0c96bb039b7c450e/fairseq/models/transformer.py#L378), [code](https://github.com/layer6ai-labs/T-Fixup/blob/f1fae213ce7b48829f81632d0c96bb039b7c450e/fairseq/models/transformer.py#L604)\n",
    "\n",
    "3. Scale $v_{e}$ and $w_{e}$ matrices in each encoder attention block and weight matrices in each encoder MLP block by $0.67N^{−\\frac{1}{4}}$: [code](https://github.com/layer6ai-labs/T-Fixup/blob/f1fae213ce7b48829f81632d0c96bb039b7c450e/fairseq/modules/transformer_layer.py#L36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222471959
    }
   },
   "outputs": [],
   "source": [
    "def trunc_normal_(x, mean=0., std=1.):\n",
    "    \"Truncated normal initialization (approximation)\"\n",
    "    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n",
    "    return x.normal_().fmod_(2).mul_(std).add_(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222472291
    }
   },
   "outputs": [],
   "source": [
    "if H.tfixup:\n",
    "    for n,p in model.named_parameters():\n",
    "        if re.match(r'.*bias$|.*bn\\.weight$|.*norm.*\\.weight',n): continue\n",
    "        gain = 1.\n",
    "        if re.match(r'.*decoder.*',n): \n",
    "            gain = (9*H.trf_dec)**(-1./4.)\n",
    "            if re.match(f'.*in_proj_weight$',n): gain *= (2**0.5)\n",
    "        elif re.match(r'.*encoder.*',n): \n",
    "            gain = 0.67*(H.trf_enc**(-1./4.))\n",
    "            if re.match(f'.*in_proj_weight$',n): gain *= (2**0.5)\n",
    "        if re.match(r'^embeds|^tagembeds', n): \n",
    "            trunc_normal_(p.data,std=(4.5*(H.trf_enc+H.trf_dec))**(-1./4.)*H.trf_dim**(-0.5))\n",
    "        else:                                  \n",
    "            nn.init.xavier_normal_(p,gain=gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222472617
    }
   },
   "outputs": [],
   "source": [
    "if H.tfixup:\n",
    "    class MyModelPatcher(ModelPatcher):\n",
    "        def new_child_module(self, child_module_name, child_module, patch_info): return nn.Identity()\n",
    "    mp = MyModelPatcher()\n",
    "    mp.add_pattern(r\".*norm\\d?.*\",{})\n",
    "    mp.patch_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222473030
    }
   },
   "outputs": [],
   "source": [
    "#noexport\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222479497
    }
   },
   "outputs": [],
   "source": [
    "dls = dls.cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222479871
    }
   },
   "outputs": [],
   "source": [
    "@delegates(Lamb)\n",
    "def ranger_lamb(p, lr, mom=0.95, wd=0.01, eps=1e-6, **kwargs):\n",
    "    return Lookahead(Lamb(p, lr=lr, mom=mom, wd=wd, eps=eps, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222480236
    }
   },
   "outputs": [],
   "source": [
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    loss_func=ua_loss_func,\n",
    "    opt_func=partial(globals()[H.opt],**H.opt_kwargs),\n",
    "    moms = H.moms,\n",
    "    metrics=[\n",
    "        LBMetric(loss_func, 'acc_valid_loss'),\n",
    "        LBMetric(roc_auc, 'acc_roc_auc'),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222480543
    }
   },
   "outputs": [],
   "source": [
    "f_fp16 = getattr(learn,H.fp16,None)\n",
    "if f_fp16: f_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222480851
    }
   },
   "outputs": [],
   "source": [
    "def rank0_only(func, *args, **kwargs):\n",
    "    \"Execute `func` in the Rank-0 process first, then in other ranks in parallel.\"\n",
    "    if args or kwargs: func = partial(func, *args, **kwargs)\n",
    "    dummy_l = Learner(DataLoaders(device='cpu'), nn.Linear(1,1), loss_func=lambda: 0)\n",
    "    res = None\n",
    "    with dummy_l.distrib_ctx():\n",
    "        if not rank_distrib(): res = func()\n",
    "        distrib_barrier()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222481471
    }
   },
   "outputs": [],
   "source": [
    "@patch\n",
    "def load(learn:Learner,fn,with_opt=False):\n",
    "    def __inner(learn:Learner,fn,with_opt=False):\n",
    "        m_dict = torch.load(f\"{(Path(learn.model_dir) / fn)}.pth\")#['model']\n",
    "        ks = []\n",
    "        for attempts in range(2):\n",
    "            try:\n",
    "                res = learn.model.load_state_dict(m_dict,strict=False)\n",
    "                print(f\"Loaded {fn} ignoring: {' '.join(ks)} and {res}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                for k in [m[1] for m in [re.match(r\"^.*mismatch for ([\\w\\.]+):\",l) for l in str(e).split(\"\\n\")] if m is not None]:\n",
    "                    m_dict.pop(k,None)\n",
    "                    ks.append(k)\n",
    "        return learn\n",
    "    return rank0_only(__inner, learn, fn, with_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222481806
    }
   },
   "outputs": [],
   "source": [
    "if H.load:\n",
    "    learn.load(H.load, with_opt=False)\n",
    "    print(f\"Loaded: {H.load}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222482189
    }
   },
   "outputs": [],
   "source": [
    "if H.clip: \n",
    "    learn.add_cb(GradientClipping(H.clip))\n",
    "    print('clip on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222482567
    }
   },
   "outputs": [],
   "source": [
    "if H.local_rank is not None: \n",
    "    learn.to_distributed(H.local_rank)\n",
    "    print('local_rank on')\n",
    "if H.mixup: \n",
    "    learn.add_cb(MyMixUp(0.5))\n",
    "    print('mixup on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222483088
    }
   },
   "outputs": [],
   "source": [
    "if H.validate:\n",
    "    res = learn.validate()\n",
    "    print(f\"CV: {res[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222485016
    }
   },
   "outputs": [],
   "source": [
    "#noexport\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222485391
    }
   },
   "outputs": [],
   "source": [
    "#noexport\n",
    "#learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222485744
    }
   },
   "outputs": [],
   "source": [
    "#noexport\n",
    "#learn.recorder.plot_lr_find(skip_end=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222486069
    }
   },
   "outputs": [],
   "source": [
    "#short_cb = learn.add_cb(ShortEpochCallback(pct=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222486459
    }
   },
   "outputs": [],
   "source": [
    "learn.add_cb(SaveModelCallback(monitor='acc_roc_auc', comp=np.greater, fname=f'best{H.model}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222486746
    }
   },
   "outputs": [],
   "source": [
    "print(H.fit, H.epochs, H.lr, H.fit_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222488701
    }
   },
   "outputs": [],
   "source": [
    "#noexport\n",
    "syn_learn = synth_learner()\n",
    "getattr(syn_learn,H.fit)(H.epochs, H.lr,**H.fit_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222489013
    }
   },
   "outputs": [],
   "source": [
    "#noexport\n",
    "syn_learn.recorder.plot_sched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222489412
    }
   },
   "outputs": [],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638222489728
    }
   },
   "outputs": [],
   "source": [
    "#learn.fit_flat_cos(H.epochs, H.lr,**H.fit_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638240814371
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Fitting {H.local_rank}\")\n",
    "getattr(learn,H.fit)(H.epochs, H.lr,**H.fit_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638240814665
    }
   },
   "outputs": [],
   "source": [
    "#noexport\n",
    "learn.recorder.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638240815164
    }
   },
   "outputs": [],
   "source": [
    "#noexport\n",
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1638240815451
    }
   },
   "outputs": [],
   "source": [
    "print(\"--- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "azureml_py38_pytorch"
  },
  "kernelspec": {
   "display_name": "riiid",
   "language": "python",
   "name": "riiid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
