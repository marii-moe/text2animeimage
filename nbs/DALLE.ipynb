{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96334584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp dalle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28a5c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link 'dall_e/dall_e': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!ln -s ../DALL_E/dall_e dall_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47515bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,ImageFile\n",
    "from pathlib import Path\n",
    "from fastai.basics import *\n",
    "from fastai.text.all import *\n",
    "from transformers import ReformerModelWithLMHead, ReformerTokenizerFast,ReformerConfig\n",
    "from fastai.callback.wandb import *\n",
    "import wandb\n",
    "from text2anime.data_loading import *\n",
    "from text2anime.reformer import *\n",
    "from dall_e import map_pixels, unmap_pixels, load_model\n",
    "Image.LOAD_TRUNCATED_IMAGES=True\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES=True\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9bf076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarii\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\n",
      "InvalidVersionSpec: Invalid version '2.3.0>=2.1.0': invalid character(s)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.32<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">worthy-haze-16</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/marii/text2animeimage-nbs\" target=\"_blank\">https://wandb.ai/marii/text2animeimage-nbs</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/marii/text2animeimage-nbs/runs/1y7gq3hg\" target=\"_blank\">https://wandb.ai/marii/text2animeimage-nbs/runs/1y7gq3hg</a><br/>\n",
       "                Run data is saved locally in <code>/home/molly/Projects/text2animeimage/nbs/wandb/run-20211007_054736-1y7gq3hg</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(1y7gq3hg)</h1><iframe src=\"https://wandb.ai/marii/text2animeimage-nbs/runs/1y7gq3hg\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f1c8d1c3df0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93642282",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=Path('../danbooru2020')\n",
    "image_path=path/'512px' #sample\n",
    "meta=path/'meta'\n",
    "bs=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebbf1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('named_tags.pkl','rb') as f: named_tags=pickle.load(f)\n",
    "with open('id_tags.pkl','rb') as f: id_tags=pickle.load(f)\n",
    "full_table = pd.read_pickle('fulll_table.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15dfd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls=DALLE_dataloader(image_path,full_table,id_tags,max_len=1152,seq_len=64)\n",
    "dls.train.bs=bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba643e",
   "metadata": {},
   "source": [
    "These seem broken with mixed image text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda51538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to have limited dataset size\n",
    "# dls.train.idx_max=len(dls.train_ds)-1\n",
    "# dls.valid.idx_max=len(dls.valid_ds)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae588370",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_path=download_data(\"https://cdn.openai.com/dall-e/encoder.pkl\",\"dalle_encoder.pkl\")\n",
    "decoder_path=download_data(\"https://cdn.openai.com/dall-e/decoder.pkl\",\"dalle_decoder.pkl\")\n",
    "with open(encoder_path, 'rb') as f: encoder=torch.load(encoder_path, map_location=torch.device('cuda:0'))\n",
    "#with open(decoder_path, 'rb') as f: decoder=torch.load(decoder_path, map_location=torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e685baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = ReformerConfig(is_decoder=True,axial_pos_shape=(64,34))\n",
    "reformer=ReformerModelWithLMHead(configuration)\n",
    "reformer.reformer.embeddings.word_embeddings = nn.Embedding(2500+1+8192,256) #2500 for text, 1 for text-image seperator, and 8192 for image tokens\n",
    "#need 1+8192 for image tokens + seperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combined_Model(nn.Module):\n",
    "    def __init__(self,transformer,vae):\n",
    "        super().__init__()\n",
    "        self.transformer=transformer\n",
    "        self.vae=vae\n",
    "        self.transformer.lm_head=nn.Linear(512,2500+1+8192)\n",
    "    def forward(self,img,text):\n",
    "        with torch.no_grad():\n",
    "            bs=img.shape[0]\n",
    "            sep=torch.tensor([2500])[None].repeat(bs,1).cuda()\n",
    "            img=TensorText(self.vae(img).argmax( axis=1).flatten(1)+2500+1)#2500 for text, 1 for text-image sep\n",
    "            sl=text.shape[1]+img.shape[1]+1\n",
    "            pad=64*34+1-sl#2240-sl #32*32+1+1153\n",
    "            pad=torch.tensor([[0]]).repeat(bs,pad).cuda()\n",
    "            text=torch.cat((pad,text,sep,img),1)\n",
    "        return self.transformer(text[:,:-1])[0],text[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Combined_Model(reformer,encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f932d9",
   "metadata": {},
   "source": [
    "z = torch\n",
    "print(z.shape,z)\n",
    "z = F.one_hot(z, num_classes=enc.vocab_size).permute(0, 3, 1, 2).float()\n",
    "\n",
    "x_stats = dec(z).float()\n",
    "x_rec = unmap_pixels(torch.sigmoid(x_stats[:, :3]))\n",
    "x_rec = T.ToPILImage(mode='RGB')(x_rec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179d0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.train.bs=bs\n",
    "class DropOutput(Callback):\n",
    "    def after_pred(self): \n",
    "        self.learn.yb= (self.pred[1],)\n",
    "        self.learn.pred = self.pred[0]\n",
    "        \n",
    "cbs=[DropOutput(),GradientAccumulation(n_acc=1024),WandbCallback()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4de6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func=partial(Lamb,mom=0.9,sqr_mom=0.96,eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1fcd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn=Learner(dls, model, loss_func=CrossEntropyLossFlat(),cbs=cbs, opt_func=opt_func, metrics=[accuracy, Perplexity()]).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dff56f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fit\n",
      "   - before_fit     : [TrainEvalCallback, GradientAccumulation, MixedPrecision, Recorder, WandbCallback, ProgressCallback]\n",
      "  Start Epoch Loop\n",
      "     - before_epoch   : [Recorder, ProgressCallback]\n",
      "    Start Train\n",
      "       - before_train   : [TrainEvalCallback, Recorder, ProgressCallback]\n",
      "      Start Batch Loop\n",
      "         - before_batch   : [MixedPrecision]\n",
      "         - after_pred     : [DropOutput, MixedPrecision]\n",
      "         - after_loss     : [GradientAccumulation, MixedPrecision]\n",
      "         - before_backward: [MixedPrecision]\n",
      "         - before_step    : [GradientAccumulation, MixedPrecision]\n",
      "         - after_step     : [MixedPrecision]\n",
      "         - after_cancel_batch: []\n",
      "         - after_batch    : [TrainEvalCallback, Recorder, WandbCallback, ProgressCallback]\n",
      "      End Batch Loop\n",
      "    End Train\n",
      "     - after_cancel_train: [Recorder]\n",
      "     - after_train    : [Recorder, ProgressCallback]\n",
      "    Start Valid\n",
      "       - before_validate: [TrainEvalCallback, Recorder, ProgressCallback]\n",
      "      Start Batch Loop\n",
      "         - **CBs same as train batch**: []\n",
      "      End Batch Loop\n",
      "    End Valid\n",
      "     - after_cancel_validate: [Recorder]\n",
      "     - after_validate : [Recorder, ProgressCallback]\n",
      "  End Epoch Loop\n",
      "   - after_cancel_epoch: []\n",
      "   - after_epoch    : [Recorder, WandbCallback]\n",
      "End Fit\n",
      " - after_cancel_fit: []\n",
      " - after_fit      : [WandbCallback, ProgressCallback]\n"
     ]
    }
   ],
   "source": [
    "learn.show_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c3c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Learner??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988471a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c99f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandbCallback requires use of \"SaveModelCallback\" to log best model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='59997' class='' max='80475' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      74.55% [59997/80475 16:23:17<5:35:36 2.4640]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "config.num_buckets is not set. Setting config.num_buckets to 64...\n",
      "config.num_buckets is not set. Setting config.num_buckets to 64...\n",
      "config.num_buckets is not set. Setting config.num_buckets to 64...\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:617: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:617: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:811: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:617: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:617: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:811: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:617: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:811: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:811: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:617: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:811: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:617: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:617: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:811: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:617: UserWarning: Metadata Warning, tag 282 had too many entries: 4, expected 1\n",
      "  warnings.warn(\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:617: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:617: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:811: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/molly/miniconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:617: UserWarning: Metadata Warning, tag 296 had too many entries: 2, expected 1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "learn.fit_flat_cos(1,lr=0.001) #0.0135 0.012 0.01 diverges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4327d25e",
   "metadata": {},
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35245b1d",
   "metadata": {},
   "source": [
    "inputs=dls.one_batch()[0]\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU],\n",
    "        profile_memory=True, record_shapes=True) as prof:\n",
    "    with torch.no_grad():\n",
    "        encoder(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
